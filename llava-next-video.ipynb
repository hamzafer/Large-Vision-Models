{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llava hugging face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 464/464 [13:19<00:00,  1.72s/frame]\n",
      "2024-12-20 00:08:49,036 - INFO - Video has been created at data/output_captioned_video_hot_air_balloons.mp4\n",
      "2024-12-20 00:08:49,036 - INFO - Total Processing Time: 799.88 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption received: The image shows a vintage Volkswagen van parked on a rocky surface with a scenic landscape in the background. The van is red and white, and it appears to be a classic model, possibly from the 1960s or 1970s. Above the van, there are several hot air balloons floating in the sky, suggesting that the location might be a popular spot for ballooning or that the photo was taken during a ballooning event. The\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "import warnings\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Video file path\n",
    "# video_path = 'data/videos_to_process/november_leaves.mp4'\n",
    "video_path = 'data/videos_to_process/hot_air_balloons.mp4'\n",
    "\n",
    "# Output video path\n",
    "output_video_path = 'data/output_captioned_video_hot_air_balloons.mp4'\n",
    "\n",
    "# Maximum frames to process (set to None to process entire video)\n",
    "max_frames = None  # Set this to the desired number of frames to process\n",
    "\n",
    "# Load processor\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "# Specify quantization config to load model in 4-bit format\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization and Flash Attention 2\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attention_2=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def generate_caption(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    \n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True).replace('[INST]', '').replace('[/INST]', '').strip()\n",
    "    caption = caption.replace(\"What is shown in this image?\", \"\").strip()\n",
    "    \n",
    "    # Clear previous captions\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Print new caption\n",
    "    print(f\"Caption received: {caption}\")\n",
    "    \n",
    "    return caption\n",
    "\n",
    "def overlay_caption(frame, caption, position=(50, 50), font_scale=1, color=(255, 255, 255), thickness=2):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # Split the caption into chunks of 15 words each\n",
    "    words = caption.split()\n",
    "    lines = [' '.join(words[i:i+15]) for i in range(0, len(words), 15)]\n",
    "\n",
    "    x, y = position\n",
    "    for line in lines:\n",
    "        # Calculate text size for each line\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(line, font, font_scale, thickness)\n",
    "        \n",
    "        # Draw background rectangle for better visibility\n",
    "        cv2.rectangle(frame, (x - 5, y - text_height - 5), \n",
    "                      (x + text_width + 5, y + 5), \n",
    "                      (0, 0, 0), -1)\n",
    "        \n",
    "        # Put the text on the frame\n",
    "        cv2.putText(frame, line, (x, y), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # Move y-coordinate down for the next line\n",
    "        y += text_height + 10  # Space between lines\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Load video using OpenCV\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each frame of the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if max_frames is None else min(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), max_frames)\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Processing Frames\", unit=\"frame\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or (max_frames is not None and frame_count >= max_frames):\n",
    "            break\n",
    "\n",
    "        # Generate caption for the frame\n",
    "        caption = generate_caption(frame)\n",
    "\n",
    "        # Overlay the caption on the frame\n",
    "        frame_with_caption = overlay_caption(frame, caption)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame_with_caption)\n",
    "\n",
    "        frame_count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "\n",
    "logging.info(\"Video has been created at %s\", output_video_path)\n",
    "logging.info(\"Total Processing Time: %.2f seconds\", total_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 23:38:06.990887: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-19 23:38:06.999007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734647887.009118  956630 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734647887.012010  956630 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-19 23:38:07.022004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c902f0b35ab49fcadfd71ffe383f985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You may have used the wrong order for inputs. `images` should be passed before `text`. The `images` and `text` inputs will be swapped. This behavior will be deprecated in transformers v4.47.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "What is shown in this image? [/INST] The image appears to be a radar chart, also known as a spider chart, which is a type of two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. This particular chart is showing the performance of a model across various metrics, which are likely related to machine learning or artificial intelligence.\n",
      "\n",
      "The axes represent different metrics, such as \"MMM-Vet,\" \"MMM-Vet,\" \"MMM-Vet,\"\n",
      "Inference Time: 2.35 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "# Load processor\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "# Specify quantization config to load model in 4-bit format\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization and Flash Attention 2\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attention_2=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare image and text prompt\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode and print output\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# Print the time taken for inference\n",
    "print(f\"Inference Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
