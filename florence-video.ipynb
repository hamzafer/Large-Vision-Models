{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new code complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "import logging\n",
    "\n",
    "# Set HOME directory\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(checkpoint=\"microsoft/Florence-2-large-ft\", device=None):\n",
    "    \"\"\"\n",
    "    Initialize the Florence-2 model and processor.\n",
    "\n",
    "    Parameters:\n",
    "    - checkpoint: The model checkpoint to use.\n",
    "    - device: The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "    - model: The initialized model.\n",
    "    - processor: The initialized processor.\n",
    "    - device: The device used.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the model and processor\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, processor, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, model, processor, device, task, text):\n",
    "    \"\"\"\n",
    "    Process a single video frame with the specified task.\n",
    "\n",
    "    Parameters:\n",
    "    - frame: The video frame to process.\n",
    "    - model: The initialized model.\n",
    "    - processor: The initialized processor.\n",
    "    - device: The device to run the model on.\n",
    "    - task: The task to perform (e.g., \"<OD>\", \"<DETAILED_CAPTION>\", etc.).\n",
    "    - text: The text input for the task.\n",
    "\n",
    "    Returns:\n",
    "    - frame_bgr: The processed frame in BGR format.\n",
    "    \"\"\"\n",
    "    # Convert the frame to RGB and then to a PIL Image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # Preprocess the input for the model\n",
    "    prompt = task + text\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate results\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        num_beams=3\n",
    "    )\n",
    "\n",
    "    # Post-process the generated text\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    response = processor.post_process_generation(generated_text, task=task, image_size=image.size)\n",
    "\n",
    "    # Handle different tasks based on the response\n",
    "    if task in [\"<OD>\", \"<OPEN_VOCABULARY_DETECTION>\", \"<CAPTION_TO_PHRASE_GROUNDING>\", \"<REGION_PROPOSAL>\", \"<REGION_TO_CATEGORY>\", \"<REGION_TO_DESCRIPTION>\"]:\n",
    "        # Tasks that output bounding boxes\n",
    "        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "        bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        image = bounding_box_annotator.annotate(image, detections)\n",
    "        image = label_annotator.annotate(image, detections)\n",
    "\n",
    "    elif task == \"<REFERRING_EXPRESSION_SEGMENTATION>\":\n",
    "        # Task that outputs segmentation masks\n",
    "        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        image = mask_annotator.annotate(image, detections)\n",
    "        image = label_annotator.annotate(image, detections)\n",
    "\n",
    "    elif task in [\"<OCR_WITH_REGION>\"]:\n",
    "        # Task that outputs OCR results with regions\n",
    "        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "        bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX, text_scale=1.5, text_thickness=2)\n",
    "        image = bounding_box_annotator.annotate(image, detections)\n",
    "        image = label_annotator.annotate(image, detections)\n",
    "\n",
    "    elif task in [\"<OCR>\", \"<DETAILED_CAPTION>\", \"<MORE_DETAILED_CAPTION>\", \"<DENSE_REGION_CAPTION>\"]:\n",
    "        # Tasks that output text captions\n",
    "        caption = response.get(task, \"\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        font = ImageFont.load_default()\n",
    "        text_position = (10, 10)\n",
    "        draw.text(text_position, caption, fill=\"red\", font=font)\n",
    "\n",
    "    else:\n",
    "        # For any other tasks, we can print the response or handle accordingly\n",
    "        logger.warning(f\"Unhandled task or output format for task: {task}\")\n",
    "\n",
    "    # Convert back to OpenCV format (BGR) for saving the video\n",
    "    frame_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return frame_bgr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_video_path, output_video_path, model, processor, device, task, text, frame_step=1):\n",
    "    \"\"\"\n",
    "    Process a video with the specified task.\n",
    "\n",
    "    Parameters:\n",
    "    - input_video_path: Path to the input video file.\n",
    "    - output_video_path: Path to save the processed video.\n",
    "    - model: The initialized model.\n",
    "    - processor: The initialized processor.\n",
    "    - device: Device to run the model on.\n",
    "    - task: The task to perform (e.g., \"<OD>\", \"<DETAILED_CAPTION>\", etc.).\n",
    "    - text: The text input for the task.\n",
    "    - frame_step: Process every Nth frame (default is 1, i.e., process every frame).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        logger.error(f\"Error opening video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    logger.info(f\"Total number of frames: {frame_count}\")\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Prepare to save the processed video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps / frame_step, (width, height))\n",
    "\n",
    "    # Process frames\n",
    "    frame_idx = 0\n",
    "    processed_frames = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_step == 0:\n",
    "            logger.info(f\"Processing frame {frame_idx+1}/{frame_count}\")\n",
    "            processed_frame = process_frame(frame, model, processor, device, task, text)\n",
    "            processed_frames += 1\n",
    "        else:\n",
    "            processed_frame = frame  # Use the original frame if not processing\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(processed_frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    logger.info(f\"Processed video saved to: {output_video_path}\")\n",
    "    logger.info(f\"Total processed frames: {processed_frames}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(video_path, video_width=600):\n",
    "    \"\"\"\n",
    "    Display a video inside the notebook.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file.\n",
    "    - video_width: Width of the video in the display.\n",
    "\n",
    "    Returns:\n",
    "    - HTML object to display the video.\n",
    "    \"\"\"\n",
    "    video_file = open(video_path, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + base64.b64encode(video_file).decode()\n",
    "    return HTML(f\"\"\"\n",
    "    <video width={video_width} controls>\n",
    "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and processor\n",
    "model, processor, device = initialize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = os.path.join(HOME, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the video\n",
    "video_url = \"https://videos.pexels.com/video-files/3015510/3015510-hd_1920_1080_24fps.mp4\"\n",
    "input_video_path = os.path.join(data_dir, \"hot_air_balloons.mp4\")\n",
    "!wget -q {video_url} -O {input_video_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons1.mp4\n",
      "Total processed frames: 464\n"
     ]
    }
   ],
   "source": [
    "# Define task and text\n",
    "# You can choose any task from the extracted task list\n",
    "# For example, Object Detection:\n",
    "task = \"<OD>\"\n",
    "text = \"<OD>\"\n",
    "\n",
    "# For Detailed Captioning:\n",
    "# task = \"<DETAILED_CAPTION>\"\n",
    "# text = \"\"\n",
    "\n",
    "# For Referring Expression Segmentation:\n",
    "# task = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "# text = \"person\"\n",
    "\n",
    "# Output video path\n",
    "output_video_path = os.path.join(data_dir, \"processed_hot_air_balloons.mp4\")\n",
    "\n",
    "# Process the video\n",
    "process_video(\n",
    "    input_video_path=input_video_path,\n",
    "    output_video_path=output_video_path,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    task=task,\n",
    "    text=text,\n",
    "    frame_step=1  # Process every frame; increase for faster processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task: <CAPTION_TO_PHRASE_GROUNDING> with text: 'person'\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_captiontophrasegrounding.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <DENSE_REGION_CAPTION> with text: ''\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_denseregioncaption.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <REGION_PROPOSAL> with text: ''\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_regionproposal.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <OCR_WITH_REGION> with text: ''\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_ocrwithregion.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <REFERRING_EXPRESSION_SEGMENTATION> with text: 'person'\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_referringexpressionsegmentation.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <REGION_TO_SEGMENTATION> with text: ''\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_regiontosegmentation.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <OPEN_VOCABULARY_DETECTION> with text: 'person'\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_openvocabularydetection.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <REGION_TO_CATEGORY> with text: ''\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_regiontocategory.mp4\n",
      "Total processed frames: 464\n",
      "Processing task: <REGION_TO_DESCRIPTION> with text: ''\n",
      "Total number of frames: 464\n",
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/processed_hot_air_balloons_regiontodescription.mp4\n",
      "Total processed frames: 464\n"
     ]
    }
   ],
   "source": [
    "# Define a list of tasks and corresponding texts\n",
    "# Supported tasks list\n",
    "tasks = [\n",
    "    # Object Detection\n",
    "    # {\"task\": \"<OD>\", \"text\": \"<OD>\"},\n",
    "\n",
    "    # Caption to Phrase Grounding\n",
    "    {\"task\": \"<CAPTION_TO_PHRASE_GROUNDING>\", \"text\": \"person\"},\n",
    "\n",
    "    # Dense Region Captioning\n",
    "    {\"task\": \"<DENSE_REGION_CAPTION>\", \"text\": \"\"},\n",
    "\n",
    "    # Region Proposal\n",
    "    {\"task\": \"<REGION_PROPOSAL>\", \"text\": \"\"},\n",
    "\n",
    "    # OCR with Region\n",
    "    {\"task\": \"<OCR_WITH_REGION>\", \"text\": \"\"},\n",
    "\n",
    "    # Referring Expression Segmentation\n",
    "    {\"task\": \"<REFERRING_EXPRESSION_SEGMENTATION>\", \"text\": \"person\"},\n",
    "\n",
    "    # Region to Segmentation\n",
    "    {\"task\": \"<REGION_TO_SEGMENTATION>\", \"text\": \"\"},\n",
    "\n",
    "    # Open Vocabulary Detection\n",
    "    {\"task\": \"<OPEN_VOCABULARY_DETECTION>\", \"text\": \"person\"},\n",
    "\n",
    "    # Region to Category\n",
    "    {\"task\": \"<REGION_TO_CATEGORY>\", \"text\": \"\"},\n",
    "\n",
    "    # Region to Description\n",
    "    {\"task\": \"<REGION_TO_DESCRIPTION>\", \"text\": \"\"}\n",
    "]\n",
    "\n",
    "# Loop through each task and process the video\n",
    "for item in tasks:\n",
    "    task = item[\"task\"]\n",
    "    text = item[\"text\"]\n",
    "    \n",
    "    # Generate a task-specific output video file name\n",
    "    task_name = task.strip(\"<>\").replace(\"_\", \"\").lower()\n",
    "    output_video_path = os.path.join(data_dir, f\"processed_hot_air_balloons_{task_name}.mp4\")\n",
    "    \n",
    "    print(f\"Processing task: {task} with text: '{text}'\")\n",
    "    \n",
    "    # Process the video\n",
    "    process_video(\n",
    "        input_video_path=input_video_path,\n",
    "        output_video_path=output_video_path,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        task=task,\n",
    "        text=text,\n",
    "        frame_step=1  # Process every frame; increase for faster processing\n",
    "    )\n",
    "    \n",
    "    # Optionally, display the processed video inside the notebook\n",
    "    # display_video(output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = os.path.join(data_dir, \"processed_hot_air_balloons.mp4\")\n",
    "\n",
    "# Display the processed video inside the notebook\n",
    "display_video(output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Usage Notes**\n",
    "\n",
    "- **Different Tasks**: You can change the `task` and `text` variables to perform different tasks. For example:\n",
    "  - **Object Detection**:\n",
    "    ```python\n",
    "    task = \"<OD>\"\n",
    "    text = \"<OD>\"\n",
    "    ```\n",
    "  - **Detailed Captioning**:\n",
    "    ```python\n",
    "    task = \"<DETAILED_CAPTION>\"\n",
    "    text = \"\"\n",
    "    ```\n",
    "  - **Referring Expression Segmentation**:\n",
    "    ```python\n",
    "    task = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "    text = \"person\"\n",
    "    ```\n",
    "  - **OCR**:\n",
    "    ```python\n",
    "    task = \"<OCR>\"\n",
    "    text = \"\"\n",
    "    ```\n",
    "  - **Open Vocabulary Detection**:\n",
    "    ```python\n",
    "    task = \"<OPEN_VOCABULARY_DETECTION>\"\n",
    "    text = \"hot air balloon\"\n",
    "    ```\n",
    "- **Processing Fewer Frames**: To process fewer frames for faster processing, increase the `frame_step` parameter in the `process_video` function. For example, `frame_step=5` will process every 5th frame.\n",
    "- **Viewing the Video**: The `display_video` function embeds the video directly into the Jupyter notebook for easy viewing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video already downloaded.\n",
      "Total number of frames: 464\n",
      "Processing frame 3/464...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 464/464...\n",
      "Processed video saved to: /home/user1/hamza/Large-Vision-Models/data/referring_expression_segmentation.mp4\n",
      "Total processed frames: 464\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Set HOME directory\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Function to initialize model and processor\n",
    "def initialize_model(checkpoint=\"microsoft/Florence-2-large-ft\", device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, processor, device\n",
    "\n",
    "# Function to run inference on an image\n",
    "def run_inference(image: Image, model, processor, device, task: str, text: str = \"\"):\n",
    "    prompt = task + text\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        num_beams=3\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    response = processor.post_process_generation(generated_text, task=task, image_size=image.size)\n",
    "    return response\n",
    "\n",
    "# Function to process a single frame\n",
    "def process_frame(frame, model, processor, device, task, text):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "\n",
    "    response = run_inference(image=image, model=model, processor=processor, device=device, task=task, text=text)\n",
    "    detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    image = mask_annotator.annotate(image, detections)\n",
    "    image = label_annotator.annotate(image, detections)\n",
    "\n",
    "    frame_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    return frame_bgr\n",
    "\n",
    "# Function to process a video\n",
    "def process_video(input_video_path, output_video_path, model, processor, device, task, text, frame_step=1):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames: {frame_count}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps / frame_step, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    processed_frames = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_step == 0:\n",
    "            print(f\"Processing frame {frame_idx+1}/{frame_count}...\", end=\"\\r\")\n",
    "            processed_frame = process_frame(frame, model, processor, device, task, text)\n",
    "            processed_frames += 1\n",
    "        else:\n",
    "            processed_frame = frame\n",
    "\n",
    "        out.write(processed_frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nProcessed video saved to: {output_video_path}\")\n",
    "    print(f\"Total processed frames: {processed_frames}\")\n",
    "\n",
    "# Main code execution\n",
    "model, processor, device = initialize_model()\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = os.path.join(HOME, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the video\n",
    "video_url = \"https://videos.pexels.com/video-files/3015510/3015510-hd_1920_1080_24fps.mp4\"\n",
    "input_video_path = os.path.join(data_dir, \"hot_air_balloons.mp4\")\n",
    "if not os.path.exists(input_video_path):\n",
    "    print(\"Downloading video...\")\n",
    "    os.system(f\"wget -q {video_url} -O {input_video_path}\")\n",
    "else:\n",
    "    print(\"Video already downloaded.\")\n",
    "\n",
    "# Define task and text\n",
    "task = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "text = \"person\"\n",
    "\n",
    "# Output video path\n",
    "output_video_path = os.path.join(data_dir, \"referring_expression_segmentation.mp4\")\n",
    "\n",
    "# Process the video\n",
    "process_video(\n",
    "    input_video_path=input_video_path,\n",
    "    output_video_path=output_video_path,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    task=task,\n",
    "    text=text,\n",
    "    frame_step=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 13:08:25.060821: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-11 13:08:25.471243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731326905.663241 1130744 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731326905.706276 1130744 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-11 13:08:26.097000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_video_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Additional prompt text can be added if needed\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Process video with captions\u001b[39;00m\n\u001b[1;32m    102\u001b[0m process_video_with_captions(\n\u001b[0;32m--> 103\u001b[0m     input_video_path\u001b[38;5;241m=\u001b[39m\u001b[43minput_video_path\u001b[49m,\n\u001b[1;32m    104\u001b[0m     output_video_path\u001b[38;5;241m=\u001b[39moutput_video_path,\n\u001b[1;32m    105\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    106\u001b[0m     processor\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[1;32m    107\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    108\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    109\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    110\u001b[0m     frame_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_video_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Set HOME directory\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Function to initialize model and processor\n",
    "def initialize_model(checkpoint=\"microsoft/Florence-2-large-ft\", device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, processor, device\n",
    "\n",
    "# Function to run inference on an image\n",
    "def run_inference(image: Image, model, processor, device, task: str, text: str = \"\"):\n",
    "    prompt = task + text\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        num_beams=3\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    response = processor.post_process_generation(generated_text, task=task, image_size=image.size)\n",
    "    return response\n",
    "\n",
    "# Function to process a single frame and overlay caption\n",
    "def process_frame(frame, model, processor, device, task, text):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # Generate caption for the frame\n",
    "    caption = run_inference(image=image, model=model, processor=processor, device=device, task=task, text=text)\n",
    "    \n",
    "    # Convert back to BGR after annotation\n",
    "    frame_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Overlay caption text on the frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame_bgr, caption, (50, 50), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return frame_bgr\n",
    "\n",
    "# Function to process a video with captions\n",
    "def process_video(input_video_path, output_video_path, model, processor, device, task, text, frame_step=1):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames: {frame_count}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps / frame_step, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    processed_frames = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_step == 0:\n",
    "            print(f\"Processing frame {frame_idx+1}/{frame_count}...\", end=\"\\r\")\n",
    "            processed_frame = process_frame(frame, model, processor, device, task, text)\n",
    "            processed_frames += 1\n",
    "        else:\n",
    "            processed_frame = frame\n",
    "\n",
    "        out.write(processed_frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nProcessed video saved to: {output_video_path}\")\n",
    "    print(f\"Total processed frames: {processed_frames}\")\n",
    "\n",
    "# Main code execution\n",
    "model, processor, device = initialize_model()\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = os.path.join(HOME, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the video\n",
    "video_url = \"https://videos.pexels.com/video-files/3015510/3015510-hd_1920_1080_24fps.mp4\"\n",
    "input_video_path = os.path.join(data_dir, \"hot_air_balloons.mp4\")\n",
    "if not os.path.exists(input_video_path):\n",
    "    print(\"Downloading video...\")\n",
    "    os.system(f\"wget -q {video_url} -O {input_video_path}\")\n",
    "else:\n",
    "    print(\"Video already downloaded.\")\n",
    "\n",
    "# Define task and text for captioning\n",
    "task = \"<DETAILED_CAPTION>\"\n",
    "text = \"\"  # Additional prompt text can be added if needed\n",
    "\n",
    "# Output video path\n",
    "output_video_path = os.path.join(data_dir, \"detailed_caption_output.mp4\")\n",
    "\n",
    "# Process video with captions\n",
    "process_video(\n",
    "    input_video_path=input_video_path,\n",
    "    output_video_path=output_video_path,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    task=task,\n",
    "    text=text,\n",
    "    frame_step=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
